{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# 07 - Comprehensive Evaluation\n", "\n", "Compare all trained models: Baseline, Soft, Hard, and Hybrid."]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": ["import os\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import torch\n", "import torch.nn.functional as F\n", "from tqdm.auto import tqdm\n", "from pathlib import Path\n", "from skimage.metrics import structural_similarity as ssim\n", "\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Load Models"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": ["# Create 4 separate model instances\n", "model_baseline = NeRF().to(device)\n", "model_soft = NeRF().to(device)\n", "model_hard = NeRF().to(device)\n", "model_hybrid = NeRF().to(device)\n", "\n", "# Load trained weights\n", "model_baseline.load_state_dict(torch.load('results/baseline/model_baseline.pth', map_location=device))\n", "model_soft.load_state_dict(torch.load('results/soft/model_soft.pth', map_location=device))\n", "model_hard.load_state_dict(torch.load('results/hard/model_hard.pth', map_location=device))\n", "model_hybrid.load_state_dict(torch.load('results/hybrid/model_hybrid.pth', map_location=device))\n", "\n", "model_baseline.eval()\n", "model_soft.eval()\n", "model_hard.eval()\n", "model_hybrid.eval()\n", "\n", "print('✅ All models loaded')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Evaluation Functions"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": ["def compute_psnr(img1, img2):\n", "    mse = np.mean((img1 - img2) ** 2)\n", "    if mse < 1e-10:\n", "        return 100.0\n", "    return -10 * np.log10(mse)\n", "\n", "def compute_ssim(img1, img2):\n", "    return ssim(img1, img2, multichannel=True, data_range=1.0, channel_axis=2)\n", "\n", "@torch.no_grad()\n", "def render_full_image(model, c2w, H, W, focal, chunk=4096):\n", "    rays_o, rays_d = get_rays(H, W, focal, torch.from_numpy(c2w).float().to(device))\n", "    rays_o = rays_o.view(-1, 3)\n", "    rays_d = rays_d.view(-1, 3)\n", "    \n", "    rgb_chunks = []\n", "    for i in range(0, rays_o.shape[0], chunk):\n", "        rgb, _ = render_rays(model, rays_o[i:i+chunk], rays_d[i:i+chunk],\n", "                            near=2.0, far=6.0, n_samples=128, perturb=False)\n", "        rgb_chunks.append(rgb)\n", "    \n", "    img = torch.cat(rgb_chunks, 0).view(H, W, 3).cpu().numpy()\n", "    return img"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Run Evaluation on Validation Set"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": ["models = {\n", "    'Baseline': model_baseline,\n", "    'Soft': model_soft,\n", "    'Hard': model_hard,\n", "    'Hybrid': model_hybrid\n", "}\n", "\n", "results = {name: {'psnr': [], 'ssim': [], 'mse': []} for name in models.keys()}\n", "\n", "print('Evaluating on validation set...')\n", "\n", "for i in tqdm(range(len(imgs_val))):\n", "    gt_img = imgs_val[i]\n", "    pose = poses_val[i]\n", "    \n", "    for name, model in models.items():\n", "        pred_img = render_full_image(model, pose, H, W, focal)\n", "        \n", "        psnr_val = compute_psnr(pred_img, gt_img)\n", "        ssim_val = compute_ssim(pred_img, gt_img)\n", "        mse_val = np.mean((pred_img - gt_img) ** 2)\n", "        \n", "        results[name]['psnr'].append(psnr_val)\n", "        results[name]['ssim'].append(ssim_val)\n", "        results[name]['mse'].append(mse_val)\n", "\n", "print('✅ Evaluation complete!')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Summary Statistics"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": ["print('\\n=== EVALUATION RESULTS ===\\n')\n", "\n", "for name in models.keys():\n", "    psnr_mean = np.mean(results[name]['psnr'])\n", "    psnr_std = np.std(results[name]['psnr'])\n", "    ssim_mean = np.mean(results[name]['ssim'])\n", "    ssim_std = np.std(results[name]['ssim'])\n", "    mse_mean = np.mean(results[name]['mse'])\n", "    \n", "    print(f'{name}:')\n", "    print(f'  PSNR: {psnr_mean:.2f} ± {psnr_std:.2f} dB')\n", "    print(f'  SSIM: {ssim_mean:.4f} ± {ssim_std:.4f}')\n", "    print(f'  MSE:  {mse_mean:.6f}')\n", "    print()"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Visualization"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": ["# Plot comparison\n", "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n", "\n", "metrics = ['psnr', 'ssim']\n", "titles = ['PSNR (dB)', 'SSIM']\n", "\n", "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n", "    ax = axes[idx]\n", "    for name in models.keys():\n", "        ax.plot(results[name][metric], label=name, alpha=0.7)\n", "    ax.set_xlabel('Validation Image')\n", "    ax.set_ylabel(title)\n", "    ax.set_title(f'{title} Comparison')\n", "    ax.legend()\n", "    ax.grid(True, alpha=0.3)\n", "\n", "# Bar plot\n", "ax = axes[2]\n", "names = list(models.keys())\n", "psnr_means = [np.mean(results[name]['psnr']) for name in names]\n", "ax.bar(names, psnr_means)\n", "ax.set_ylabel('PSNR (dB)')\n", "ax.set_title('Average PSNR')\n", "ax.grid(True, alpha=0.3, axis='y')\n", "\n", "ax = axes[3]\n", "ssim_means = [np.mean(results[name]['ssim']) for name in names]\n", "ax.bar(names, ssim_means)\n", "ax.set_ylabel('SSIM')\n", "ax.set_title('Average SSIM')\n", "ax.grid(True, alpha=0.3, axis='y')\n", "\n", "plt.tight_layout()\n", "plt.savefig('results/evaluation_comparison.png', dpi=150, bbox_inches='tight')\n", "plt.show()\n", "\n", "print('✅ Evaluation plots saved to results/evaluation_comparison.png')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Save Results"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": ["# Save numerical results\n", "np.save('results/evaluation_results.npy', results)\n", "print('✅ Results saved to results/evaluation_results.npy')"]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}