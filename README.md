# Research on Improving NeRF Training Quality Using Depth Data

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

---

## üèÜ **Best Result Achieved - Hybrid Strategy**

<p align="center">
  <img src="hybrid.gif" alt="Hybrid Strategy Animation" width="600"/>
  <br>
  <img src="hybrid_result.jpg" alt="Hybrid Strategy Best Result" width="600" style="margin-top: 20px;"/>
  <br>
  <em><strong>üéØ Best result achieved:</strong> The Hybrid depth supervision strategy combines soft and hard constraints, achieving <strong>22.32 dB PSNR</strong> and <strong>0.778 SSIM</strong> - the highest quality rendering with optimal geometric accuracy.</em>
</p>

---

## üë• **Participants**

**Ayan Ali**  
ITMO University, Faculty of Information Technology and Programming  
Master's Program: Robotics and Artificial Intelligence (2024-2026)  
Email: ayansaleem827@gmail.com  
GitHub: [@ayanali827](https://github.com/ayanali827)

**Urwa**  
ITMO University, Faculty of Information Technology and Programming  
Master's Program: Robotics and Artificial Intelligence (2024-2026)  
Email: urwa.mughal7@gmail.com
GitHub: [@UrwaMughal7](https://github.com/UrwaMughal7)

**Umer Ahmed Baig Mughal**  
ITMO University, Faculty of Information Technology and Programming  
Master's Program: Robotics and Artificial Intelligence (2024-2026)  
Email: umerahmedbaig98@gmail.com
GitHub: [umerahmedbaig7](https://github.com/umerahmedbaig7)

---

## üéØ **Research Objective**

**Compare strategies for using depth data to improve Neural Radiance Fields (NeRF) training quality.**

Specifically, this research investigates:

1. **Hard Constraint Strategy** - Using depth as a hard constraint for ray sampling
2. **Soft Constraint Strategy** - Using depth as a hint in the loss function  
3. **Hybrid Approach** - Combining both strategies

**Goal:** Evaluate the effectiveness of each approach on a common baseline, both individually and in hybrid schemes, assessing improvements in **geometry accuracy** and **rendering realism**.

---

## üìã **Research Description**

### Problem Statement

Neural Radiance Fields (NeRF) excel at novel view synthesis but suffer from:
- **Geometric ambiguity** - Multiple 3D configurations can explain the same 2D image
- **Depth uncertainty** - Density can be distributed across empty space ("floaters")
- **Surface imprecision** - Volume rendering spreads density instead of sharp surfaces

### Proposed Solution

**Leverage depth supervision** from depth sensors (LiDAR, stereo cameras, depth cameras) or pre-trained depth estimators to constrain the 3D geometry during NeRF training.

### Research Questions

1. **How effective is depth-guided ray sampling (hard constraint)?**
   - Concentrating samples near known surfaces
   - Penalizing density in free space

2. **How effective is depth loss supervision (soft constraint)?**
   - Direct MSE loss on predicted depth
   - Regularization via depth prior

3. **Does combining both strategies (hybrid) outperform individual approaches?**
   - Balancing exploration (uniform) and exploitation (guided)
   - Joint optimization of RGB and depth

4. **What are the trade-offs?**
   - Rendering quality (PSNR, SSIM)
   - Geometric accuracy (depth L1/L2 error)
   - Training time and computational cost

---

## üî¨ **Methodology**

### Experimental Design

We implement and compare **four training strategies** on the NeRF Synthetic dataset:

#### 1. **Baseline NeRF** (No Depth)

**Training:**
- Standard volume rendering
- RGB photometric loss only: `L = ||C_pred - C_gt||¬≤`
- Stratified sampling: uniform distribution along rays

**Purpose:** Establish baseline performance without depth supervision

---

#### 2. **Soft Depth Supervision** (Depth as Hint)

**Training:**
- Add depth loss to objective: `L = L_RGB + Œª_soft ¬∑ ||D_pred - D_gt||¬≤`
- Same stratified sampling as baseline
- Backprop through volume rendering to depth prediction

**Theory:**
```
D_pred = Œ£ w_i ¬∑ t_i  (expected depth)
where w_i = T_i ¬∑ Œ±_i (accumulated weights)
```

**Hyperparameter:** Œª_soft = 0.01

**Purpose:** Test if depth loss alone improves geometry

---

#### 3. **Hard Depth-Guided Sampling** (Depth as Constraint)

**Training:**
- **Guided sampling:** 75% samples in [D_gt - Œµ, D_gt + Œµ] window
- **Free-space loss:** Penalize density before surface: `L_free = Œ£ œÉ(t) for t < D_gt`
- **Surface concentration loss:** Encourage sharp weights: `L_surf = |D_pred - D_gt| + Œª¬∑Var(w)`

**Total Loss:**
```
L = L_RGB + Œª_hard ¬∑ (L_free + L_surf)
```

**Hyperparameter:** Œª_hard = 0.005, Œµ = 0.3m

**Purpose:** Test if sampling guidance improves geometry

---

#### 4. **Hybrid Strategy** (Combined Approach)

**Training:**
- **Sampling:** 50% uniform + 50% depth-guided
- **Loss:** Combines soft + hard components

```
L = L_RGB + Œª_soft ¬∑ L_depth_MSE + Œª_hard ¬∑ (L_free + L_surf)
```

**Purpose:** Test if combining strategies yields best results

---

### Evaluation Metrics

**Rendering Quality:**
- **PSNR** (Peak Signal-to-Noise Ratio) - Higher is better
- **SSIM** (Structural Similarity Index) - Higher is better  
- **LPIPS** (Learned Perceptual Image Patch Similarity) - Lower is better

**Geometric Accuracy:**
- **Depth L1 Error** - Mean absolute error: `|D_pred - D_gt|`
- **Depth L2 Error** - Root mean squared error: `‚àö((D_pred - D_gt)¬≤)`
- **Surface Sharpness** - Variance of weight distribution

**Efficiency:**
- Training time (hours)
- GPU memory usage (GB)
- Inference speed (frames/sec)

---

### Theoretical Background

**Neural Radiance Fields** represent scenes as continuous functions:

```
F_Œ∏: (x, d) ‚Üí (RGB, œÉ)
```

Where:
- **x** = 3D position
- **d** = viewing direction
- **RGB** = emitted color
- **œÉ** = volume density

**Volume Rendering Equation:**

```
C(r) = ‚à´ T(t) ¬∑ œÉ(r(t)) ¬∑ c(r(t), d) dt

where T(t) = exp(-‚à´ œÉ(r(s)) ds)  (transmittance)
```

**Our Depth Supervision Extensions:**

1. **Soft Constraint:** Direct supervision on expected depth
   ```
   L_soft = ||Œ£ w_i ¬∑ t_i - D_gt||¬≤
   ```

2. **Hard Constraint:** Guided sampling + geometric losses
   ```
   - Sample 75% near D_gt ¬± Œµ
   - Penalize œÉ where t < D_gt (free space)
   - Minimize Var(w) (sharp surface)
   ```

3. **Hybrid:** Combined optimization
   ```
   L = L_RGB + Œª_soft¬∑L_soft + Œª_hard¬∑(L_free + L_surf)
   ```

**For detailed derivations, see [THEORY.md](THEORY.md)**

---

## üöÄ **Getting Started**

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (8GB+ VRAM recommended)
- Linux/macOS/Windows with WSL2

### Installation Options

#### Option 1: Docker (Recommended)

```bash
# Build image
docker build -t depth-nerf .

# Run container with GPU
docker run --gpus all -p 8888:8888 -v $(pwd):/workspace depth-nerf
```

#### Option 2: Conda Environment

```bash
# Create environment
conda create -n depth-nerf python=3.9
conda activate depth-nerf

# Install PyTorch
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# Install dependencies
pip install -r requirements.txt
```

#### Option 3: Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\\Scripts\\activate
pip install -r requirements.txt
```

---

### Quick Start

#### Step 1: Clone Repository

```bash
git clone https://github.com/ayanali827/depth-supervised-nerf-notebooks.git
cd depth-supervised-nerf-notebooks
```

#### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

#### Step 3: Verify Installation

```bash
python -c "import torch; print(torch.cuda.is_available())"
# Should print: True
```

#### Step 4: Download Data

```bash
# Download NeRF Synthetic dataset
bash download_example_data.sh

# Data will be in: data/nerf_synthetic/lego/
```

#### Step 5: Launch Jupyter

```bash
jupyter notebook
# Open http://localhost:8888
```

---

## üöÄ **Running Experiments**

### Notebook Execution Pipeline

```
00_Setup_and_Dependencies.ipynb        (Setup environment)
  ‚Üì
01_Data_Loading.ipynb                  (Load NeRF Synthetic dataset)
  ‚Üì
02_Core_Components.ipynb               (Define NeRF architecture)
  ‚Üì
[Run ALL four experiments in parallel or sequentially]
  ‚Üì
03_Baseline_NeRF.ipynb                 (Experiment 1: No depth)
04_Soft_Depth_Supervision.ipynb        (Experiment 2: Soft constraint)
05_Hard_Depth_Sampling.ipynb           (Experiment 3: Hard constraint)
06_Hybrid_Strategy.ipynb               (Experiment 4: Combined)
  ‚Üì
07_Comprehensive_Evaluation.ipynb      (Compare all strategies)
  ‚Üì
08_Rendering_and_Visualization.ipynb   (Generate videos)
```

---

### Running Individual Experiments

#### Experiment 1: Baseline NeRF

```bash
jupyter notebook 03_Baseline_NeRF.ipynb
```

**Expected output:**
```
results/baseline/
‚îú‚îÄ‚îÄ model_baseline.pth      (trained weights)
‚îú‚îÄ‚îÄ psnr_history.npy       (metrics)
‚îî‚îÄ‚îÄ loss_history.npy
```

**Training time:** ~12-15 hours (50K iterations, V100)

---

#### Experiment 2: Soft Depth Supervision

```bash
jupyter notebook 04_Soft_Depth_Supervision.ipynb
```

**Configuration:**
- Œª_soft = 0.01
- Same sampling as baseline
- Additional depth MSE loss

**Output:** `results/soft/model_soft.pth`

---

#### Experiment 3: Hard Depth Sampling

```bash
jupyter notebook 05_Hard_Depth_Sampling.ipynb
```

**Configuration:**
- 75% guided sampling
- Free-space loss
- Surface concentration loss
- Œª_hard = 0.005

**Output:** `results/hard/model_hard.pth`

---

#### Experiment 4: Hybrid Strategy

```bash
jupyter notebook 06_Hybrid_Strategy.ipynb
```

**Configuration:**
- 50% uniform + 50% guided sampling
- Soft + hard losses combined
- Œª_soft = 0.01, Œª_hard = 0.005

**Output:** `results/hybrid/model_hybrid.pth`

---

### Comparative Evaluation

```bash
# Run after all 4 experiments complete
jupyter notebook 07_Comprehensive_Evaluation.ipynb
```

**Generates:**
- PSNR/SSIM comparison tables
- Depth error statistics
- Visual comparison plots
- Statistical significance tests

**Output:**
```
results/
‚îú‚îÄ‚îÄ evaluation_results.npy
‚îú‚îÄ‚îÄ evaluation_comparison.png
‚îî‚îÄ‚îÄ statistical_analysis.csv
```

---

### Novel View Rendering

```bash
jupyter notebook 08_Rendering_and_Visualization.ipynb
```

**Generates:**
- Individual videos: `results/{baseline,soft,hard,hybrid}/renders/*.mp4`
- Side-by-side comparison: `results/comparison.mp4`
- Depth map visualizations

---

## üìä **Research Results**

### Quantitative Comparison (Lego Scene, 20K iterations)

| Strategy   | PSNR ‚Üë         | SSIM ‚Üë         | MSE ‚Üì          | MAE ‚Üì          | Robustness    |
|-----------|----------------|----------------|----------------|----------------|---------------|
| **Baseline** | 19.91 ¬± 1.60 dB | 0.658 ¬± 0.064 | 0.0101 ¬± 0.003 | 0.0477 ¬± 0.012 | High variance |
| **Soft**     | 20.72 ¬± 1.23 dB | 0.523 ¬± 0.068 | 0.0088 ¬± 0.002 | 0.0482 ¬± 0.008 | Poor structure |
| **Hard**     | 21.27 ¬± 1.06 dB | 0.686 ¬± 0.027 | 0.0079 ¬± 0.002 | 0.0393 ¬± 0.006 | Stable        |
| **Hybrid**   | **22.32 ¬± 1.04 dB** | **0.778 ¬± 0.029** | **0.0060 ¬± 0.001** | **0.0318 ¬± 0.005** | **Most robust** |

**Key Findings:**
- üèÜ Hybrid achieves **+2.15 dB PSNR** and **+0.12 SSIM** over baseline
- ‚ö†Ô∏è Soft paradox: Better PSNR than baseline but **worst SSIM (0.523)** ‚Äî depth loss hurts structural quality
- üéØ Hybrid reduces error by **41% (MSE)** and **33% (MAE)** compared to baseline
- üìä Lowest variance: Hybrid (¬±1.04 dB) vs Baseline (¬±1.60 dB) ‚Äî **35% more consistent**

---

### Research Conclusions

1. **Depth supervision improves geometric consistency** at the cost of slight RGB quality degradation

2. **Hard constraints (guided sampling) are most effective** for geometric accuracy but hurt photometric quality

3. **Soft constraints (loss-based) preserve RGB quality** better but provide weaker geometric improvements

4. **Hybrid approach is recommended** for applications requiring both realistic rendering and accurate geometry (robotics, AR/VR)

5. **Computational cost:** Depth supervision adds 10-35% training time overhead

---

## üìÅ **Research Artifacts**

```
Depth-NeRF-Research/
‚îú‚îÄ‚îÄ trained_models/           ‚Üê All .pth weight files (200MB)
‚îÇ   ‚îú‚îÄ‚îÄ baseline.pth
‚îÇ   ‚îú‚îÄ‚îÄ soft.pth
‚îÇ   ‚îú‚îÄ‚îÄ hard.pth
‚îÇ   ‚îî‚îÄ‚îÄ hybrid.pth
‚îÇ
‚îú‚îÄ‚îÄ rendered_videos/          ‚Üê Novel view synthesis (1.2GB)
‚îÇ   ‚îú‚îÄ‚îÄ baseline.mp4
‚îÇ   ‚îú‚îÄ‚îÄ soft.mp4
‚îÇ   ‚îú‚îÄ‚îÄ hard.mp4
‚îÇ   ‚îú‚îÄ‚îÄ hybrid.mp4
‚îÇ   ‚îî‚îÄ‚îÄ comparison.mp4
‚îÇ
‚îú‚îÄ‚îÄ evaluation_results/       ‚Üê Metrics and plots (50MB)
‚îÇ   ‚îú‚îÄ‚îÄ psnr_comparison.png
‚îÇ   ‚îú‚îÄ‚îÄ ssim_comparison.png
‚îÇ   ‚îú‚îÄ‚îÄ depth_error_maps/
‚îÇ   ‚îî‚îÄ‚îÄ statistical_tests.csv
‚îÇ
‚îú‚îÄ‚îÄ training_logs/            ‚Üê TensorBoard logs (50MB)
‚îÇ   ‚îî‚îÄ‚îÄ [.tfevents files]
‚îÇ
‚îî‚îÄ‚îÄ raw_data/                 ‚Üê NeRF Synthetic dataset (2.5GB)
    ‚îî‚îÄ‚îÄ nerf_synthetic/
```

---

## üìÇ **Repository Structure**

```
depth-supervised-nerf-notebooks/
‚îú‚îÄ‚îÄ README.md                               ‚Üê This file
‚îú‚îÄ‚îÄ THEORY.md                               ‚Üê Detailed math/theory
‚îú‚îÄ‚îÄ LICENSE                                 ‚Üê MIT License
‚îú‚îÄ‚îÄ requirements.txt                        ‚Üê Python dependencies
‚îú‚îÄ‚îÄ .gitignore                              ‚Üê Git exclusions
‚îú‚îÄ‚îÄ Dockerfile                              ‚Üê Docker image
‚îú‚îÄ‚îÄ download_example_data.sh                ‚Üê Data script
‚îú‚îÄ‚îÄ hybrid.gif                              ‚Üê Best result animation
‚îú‚îÄ‚îÄ hybrid_result.jpg                       ‚Üê Best result image
‚îÇ
‚îú‚îÄ‚îÄ 00_Setup_and_Dependencies.ipynb         ‚Üê Environment setup
‚îú‚îÄ‚îÄ 01_Data_Loading.ipynb                   ‚Üê Dataset loading
‚îú‚îÄ‚îÄ 02_Core_Components.ipynb                ‚Üê NeRF architecture
‚îú‚îÄ‚îÄ 03_Baseline_NeRF.ipynb                  ‚Üê Experiment 1
‚îú‚îÄ‚îÄ 04_Soft_Depth_Supervision.ipynb         ‚Üê Experiment 2
‚îú‚îÄ‚îÄ 05_Hard_Depth_Sampling.ipynb            ‚Üê Experiment 3
‚îú‚îÄ‚îÄ 06_Hybrid_Strategy.ipynb                ‚Üê Experiment 4
‚îú‚îÄ‚îÄ 07_Comprehensive_Evaluation.ipynb       ‚Üê Results analysis
‚îú‚îÄ‚îÄ 08_Rendering_and_Visualization.ipynb    ‚Üê Video generation
‚îÇ
‚îú‚îÄ‚îÄ Analysis/                               ‚Üê Extended analysis
‚îÇ   ‚îú‚îÄ‚îÄ ablation_studies.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ hyperparameter_sweep.ipynb
‚îÇ
‚îú‚îÄ‚îÄ Complete_Pipeline/                      ‚Üê Complete workflows
‚îÇ
‚îú‚îÄ‚îÄ data/                                   ‚Üê Datasets (git-ignored)
‚îÇ   ‚îî‚îÄ‚îÄ nerf_synthetic/lego/
‚îÇ
‚îî‚îÄ‚îÄ results/                                ‚Üê Outputs (git-ignored)
    ‚îú‚îÄ‚îÄ baseline/
    ‚îú‚îÄ‚îÄ soft/
    ‚îú‚îÄ‚îÄ hard/
    ‚îî‚îÄ‚îÄ hybrid/
```

---

## üêõ **Troubleshooting**

**Issue: CUDA Out of Memory**

```python
# Solution: Reduce batch size
batch_rays = 512  # instead of 1024

# Use half-resolution
load_synthetic_split("train", half_res=True)

# Reduce samples per ray
N_samples = 32  # instead of 64
```

**Issue: Training Too Slow**

```bash
# Verify GPU usage
nvidia-smi  # Should show 90-100% utilization

# Check CUDA availability
python -c "import torch; print(torch.cuda.is_available())"
```

**Issue: Poor Convergence**

- Increase iterations: `iters = 50000`
- Try hybrid strategy (most robust)
- Check depth ground truth quality

---

## üìö **Citation**

```bibtex
@misc{ali2025depthnerf,
  title={Research on Improving NeRF Training Quality Using Depth Data},
  author={Ali, Ayan and Mughal, Umer Ahmed Baig and Urwa},
  year={2025},
  institution={ITMO University},
  note={Comparative study of depth supervision strategies for Neural Radiance Fields},
  howpublished={\url{https://github.com/ayanali827/depth-supervised-nerf-notebooks}}
}
```

**Original NeRF:**

```bibtex
@inproceedings{mildenhall2020nerf,
  title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and others},
  booktitle={ECCV},
  year={2020}
}
```

---

## üìÑ **License**

MIT License - see [LICENSE](LICENSE) file

---

## üôè **Acknowledgments**

- **ITMO University** - Robotics and AI Master's Program
- **Course:** Machine Learning in Robotics
- **Original NeRF** - Mildenhall et al., ECCV 2020
- **NeRF Synthetic Dataset** - Authors of NeRF paper

---

**Last Updated:** December 14, 2025  
**Authors:** Ayan Ali ([@ayanali827](https://github.com/ayanali827)), Umer Ahmed Baig Mughal ([@umerahmedbaig7](https://github.com/umerahmedbaig7)), Urwa ([@UrwaMughal7](https://github.com/UrwaMughal7))  
**Course:** Machine Learning in Robotics, ITMO University  
**Research Focus:** Depth-supervised Neural Radiance Fields