# Research on Improving NeRF Training Quality Using Depth Data

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

---
## ğŸ‘¥ **Participants**

**Ayan Ali**  
ITMO University, Faculty of Information Technology and Programming  
Master's Program: Robotics and Artificial Intelligence (2024-2026)  
Email: ayansaleem827@gmail.com  
GitHub: [@ayanali827](https://github.com/ayanali827)

**Urwa**  
ITMO University, Faculty of Information Technology and Programming  
Master's Program: Robotics and Artificial Intelligence (2024-2026)  
Email: urwa.mughal7@gmail.com
GitHub: [@UrwaMughal7](https://github.com/UrwaMughal7)

**Umer Ahmed Baig Mughal**  
ITMO University, Faculty of Information Technology and Programming  
Master's Program: Robotics and Artificial Intelligence (2024-2026)  
Email: umerahmedbaig98@gmail.com
GitHub: [umerahmedbaig7](https://github.com/umerahmedbaig7)

---

## ğŸ¯ **Research Objective**

**Compare strategies for using depth data to improve Neural Radiance Fields (NeRF) training quality.**

Specifically, this research investigates:

1. **Hard Constraint Strategy** - Using depth as a hard constraint for ray sampling
2. **Soft Constraint Strategy** - Using depth as a hint in the loss function  
3. **Hybrid Approach** - Combining both strategies

**Goal:** Evaluate the effectiveness of each approach on a common baseline, both individually and in hybrid schemes, assessing improvements in **geometry accuracy** and **rendering realism**.

---

## ğŸ“‹ **Research Description**

### Problem Statement

Neural Radiance Fields (NeRF) excel at novel view synthesis but suffer from:
- **Geometric ambiguity** - Multiple 3D configurations can explain the same 2D image
- **Depth uncertainty** - Density can be distributed across empty space ("floaters")
- **Surface imprecision** - Volume rendering spreads density instead of sharp surfaces

### Proposed Solution

**Leverage depth supervision** from depth sensors (LiDAR, stereo cameras, depth cameras) or pre-trained depth estimators to constrain the 3D geometry during NeRF training.

### Research Questions

1. **How effective is depth-guided ray sampling (hard constraint)?**
   - Concentrating samples near known surfaces
   - Penalizing density in free space

2. **How effective is depth loss supervision (soft constraint)?**
   - Direct MSE loss on predicted depth
   - Regularization via depth prior

3. **Does combining both strategies (hybrid) outperform individual approaches?**
   - Balancing exploration (uniform) and exploitation (guided)
   - Joint optimization of RGB and depth

4. **What are the trade-offs?**
   - Rendering quality (PSNR, SSIM)
   - Geometric accuracy (depth L1/L2 error)
   - Training time and computational cost

---

## ğŸ”¬ **Methodology**

### Experimental Design

We implement and compare **four training strategies** on the NeRF Synthetic dataset:

#### 1. **Baseline NeRF** (No Depth)

**Training:**
- Standard volume rendering
- RGB photometric loss only: `L = ||C_pred - C_gt||Â²`
- Stratified sampling: uniform distribution along rays

**Purpose:** Establish baseline performance without depth supervision

---

#### 2. **Soft Depth Supervision** (Depth as Hint)

**Training:**
- Add depth loss to objective: `L = L_RGB + Î»_soft Â· ||D_pred - D_gt||Â²`
- Same stratified sampling as baseline
- Backprop through volume rendering to depth prediction

**Theory:**
```
D_pred = Î£ w_i Â· t_i  (expected depth)
where w_i = T_i Â· Î±_i (accumulated weights)
```

**Hyperparameter:** Î»_soft = 0.01

**Purpose:** Test if depth loss alone improves geometry

---

#### 3. **Hard Depth-Guided Sampling** (Depth as Constraint)

**Training:**
- **Guided sampling:** 75% samples in [D_gt - Îµ, D_gt + Îµ] window
- **Free-space loss:** Penalize density before surface: `L_free = Î£ Ïƒ(t) for t < D_gt`
- **Surface concentration loss:** Encourage sharp weights: `L_surf = |D_pred - D_gt| + Î»Â·Var(w)`

**Total Loss:**
```
L = L_RGB + Î»_hard Â· (L_free + L_surf)
```

**Hyperparameter:** Î»_hard = 0.005, Îµ = 0.3m

**Purpose:** Test if sampling guidance improves geometry

---

#### 4. **Hybrid Strategy** (Combined Approach)

**Training:**
- **Sampling:** 50% uniform + 50% depth-guided
- **Loss:** Combines soft + hard components

```
L = L_RGB + Î»_soft Â· L_depth_MSE + Î»_hard Â· (L_free + L_surf)
```

**Purpose:** Test if combining strategies yields best results

---

### Evaluation Metrics

**Rendering Quality:**
- **PSNR** (Peak Signal-to-Noise Ratio) - Higher is better
- **SSIM** (Structural Similarity Index) - Higher is better  
- **LPIPS** (Learned Perceptual Image Patch Similarity) - Lower is better

**Geometric Accuracy:**
- **Depth L1 Error** - Mean absolute error: `|D_pred - D_gt|`
- **Depth L2 Error** - Root mean squared error: `âˆš((D_pred - D_gt)Â²)`
- **Surface Sharpness** - Variance of weight distribution

**Efficiency:**
- Training time (hours)
- GPU memory usage (GB)
- Inference speed (frames/sec)

---

### Theoretical Background

**Neural Radiance Fields** represent scenes as continuous functions:

```
F_Î¸: (x, d) â†’ (RGB, Ïƒ)
```

Where:
- **x** = 3D position
- **d** = viewing direction
- **RGB** = emitted color
- **Ïƒ** = volume density

**Volume Rendering Equation:**

```
C(r) = âˆ« T(t) Â· Ïƒ(r(t)) Â· c(r(t), d) dt

where T(t) = exp(-âˆ« Ïƒ(r(s)) ds)  (transmittance)
```

**Our Depth Supervision Extensions:**

1. **Soft Constraint:** Direct supervision on expected depth
   ```
   L_soft = ||Î£ w_i Â· t_i - D_gt||Â²
   ```

2. **Hard Constraint:** Guided sampling + geometric losses
   ```
   - Sample 75% near D_gt Â± Îµ
   - Penalize Ïƒ where t < D_gt (free space)
   - Minimize Var(w) (sharp surface)
   ```

3. **Hybrid:** Combined optimization
   ```
   L = L_RGB + Î»_softÂ·L_soft + Î»_hardÂ·(L_free + L_surf)
   ```

**For detailed derivations, see [THEORY.md](THEORY.md)**


#### Step 4: Download Data

```bash
# Download NeRF Synthetic dataset
bash download_example_data.sh

# Data will be in: data/nerf_synthetic/lego/
```

#### Step 5: Launch Jupyter

```bash
jupyter notebook
# Open http://localhost:8888
```

---

## ğŸš€ **Running Experiments**

### Notebook Execution Pipeline

```
00_Setup_and_Dependencies.ipynb        (Setup environment)
  â†“
01_Data_Loading.ipynb                  (Load NeRF Synthetic dataset)
  â†“
02_Core_Components.ipynb               (Define NeRF architecture)
  â†“
[Run ALL four experiments in parallel or sequentially]
  â†“
03_Baseline_NeRF.ipynb                 (Experiment 1: No depth)
04_Soft_Depth_Supervision.ipynb        (Experiment 2: Soft constraint)
05_Hard_Depth_Sampling.ipynb           (Experiment 3: Hard constraint)
06_Hybrid_Strategy.ipynb               (Experiment 4: Combined)
  â†“
07_Comprehensive_Evaluation.ipynb      (Compare all strategies)
  â†“
08_Rendering_and_Visualization.ipynb   (Generate videos)
```

---

### Running Individual Experiments

#### Experiment 1: Baseline NeRF

```bash
jupyter notebook 03_Baseline_NeRF.ipynb
```

**Expected output:**
```
results/baseline/
â”œâ”€â”€ model_baseline.pth      (trained weights)
â”œâ”€â”€ psnr_history.npy       (metrics)
â””â”€â”€ loss_history.npy
```

**Training time:** ~12-15 hours (50K iterations, V100)

---

#### Experiment 2: Soft Depth Supervision

```bash
jupyter notebook 04_Soft_Depth_Supervision.ipynb
```

**Configuration:**
- Î»_soft = 0.01
- Same sampling as baseline
- Additional depth MSE loss

**Output:** `results/soft/model_soft.pth`

---

#### Experiment 3: Hard Depth Sampling

```bash
jupyter notebook 05_Hard_Depth_Sampling.ipynb
```

**Configuration:**
- 75% guided sampling
- Free-space loss
- Surface concentration loss
- Î»_hard = 0.005

**Output:** `results/hard/model_hard.pth`

---

#### Experiment 4: Hybrid Strategy

```bash
jupyter notebook 06_Hybrid_Strategy.ipynb
```

**Configuration:**
- 50% uniform + 50% guided sampling
- Soft + hard losses combined
- Î»_soft = 0.01, Î»_hard = 0.005

**Output:** `results/hybrid/model_hybrid.pth`

---

### Comparative Evaluation

```bash
# Run after all 4 experiments complete
jupyter notebook 07_Comprehensive_Evaluation.ipynb
```

**Generates:**
- PSNR/SSIM comparison tables
- Depth error statistics
- Visual comparison plots
- Statistical significance tests

**Output:**
```
results/
â”œâ”€â”€ evaluation_results.npy
â”œâ”€â”€ evaluation_comparison.png
â””â”€â”€ statistical_analysis.csv
```

---

### Novel View Rendering

```bash
jupyter notebook 08_Rendering_and_Visualization.ipynb
```

**Generates:**
- Individual videos: `results/{baseline,soft,hard,hybrid}/renders/*.mp4`
- Side-by-side comparison: `results/comparison.mp4`
- Depth map visualizations

---


<p align="center">
  <img src="assets/hybrid_best.gif" alt="Hybrid Depth-Supervised NeRF Result" width="700"/>
  <br>
  <em><strong>Best result:</strong> Hybrid depth supervision strategy on the LEGO NeRF scene.</em>
</p>


### Best Visual Result (Hybrid Strategy)

<p align="center">
  <img src="assets/hybrid_best.gif" alt="Hybrid NeRF Rendering - Best Result" width="600"/>
  <br>
  <em>Hybrid strategy achieves the best trade-off between RGB quality and depth consistency.</em>
</p>




---




## ğŸ“Š **Research Results**

### Quantitative Comparison (Lego Scene, 20K iterations)

| Strategy   | PSNR â†‘         | SSIM â†‘         | MSE â†“          | MAE â†“          | Robustness    |
|-----------|----------------|----------------|----------------|----------------|---------------|
| **Baseline** | 19.91 Â± 1.60 dB | 0.658 Â± 0.064 | 0.0101 Â± 0.003 | 0.0477 Â± 0.012 | High variance |
| **Soft**     | 20.72 Â± 1.23 dB | 0.523 Â± 0.068 | 0.0088 Â± 0.002 | 0.0482 Â± 0.008 | Poor structure |
| **Hard**     | 21.27 Â± 1.06 dB | 0.686 Â± 0.027 | 0.0079 Â± 0.002 | 0.0393 Â± 0.006 | Stable        |
| **Hybrid**   | **22.32 Â± 1.04 dB** | **0.778 Â± 0.029** | **0.0060 Â± 0.001** | **0.0318 Â± 0.005** | **Most robust** |

**Key Findings:**
- ğŸ† Hybrid achieves **+2.15 dB PSNR** and **+0.12 SSIM** over baseline
- âš ï¸ Soft paradox: Better PSNR than baseline but **worst SSIM (0.523)** â€” depth loss hurts structural quality
- ğŸ¯ Hybrid reduces error by **41% (MSE)** and **33% (MAE)** compared to baseline
- ğŸ“Š Lowest variance: Hybrid (Â±1.04 dB) vs Baseline (Â±1.60 dB) â€” **35% more consistent**


### Research Conclusions

1. **Depth supervision improves geometric consistency** at the cost of slight RGB quality degradation

2. **Hard constraints (guided sampling) are most effective** for geometric accuracy but hurt photometric quality

3. **Soft constraints (loss-based) preserve RGB quality** better but provide weaker geometric improvements

4. **Hybrid approach is recommended** for applications requiring both realistic rendering and accurate geometry (robotics, AR/VR)

5. **Computational cost:** Depth supervision adds 10-35% training time overhead

---

## ğŸ“ **Research Artifacts**

**All experimental data available at:**

ğŸ“¦ **[Google Drive - Complete Results](https://drive.google.com/drive/folders/placeholder)**

```
Depth-NeRF-Research/
â”œâ”€â”€ trained_models/           â† All .pth weight files (200MB)
â”‚   â”œâ”€â”€ baseline.pth
â”‚   â”œâ”€â”€ soft.pth
â”‚   â”œâ”€â”€ hard.pth
â”‚   â””â”€â”€ hybrid.pth
â”‚
â”œâ”€â”€ rendered_videos/          â† Novel view synthesis (1.2GB)
â”‚   â”œâ”€â”€ baseline.mp4
â”‚   â”œâ”€â”€ soft.mp4
â”‚   â”œâ”€â”€ hard.mp4
â”‚   â”œâ”€â”€ hybrid.mp4
â”‚   â””â”€â”€ comparison.mp4
â”‚
â”œâ”€â”€ evaluation_results/       â† Metrics and plots (50MB)
â”‚   â”œâ”€â”€ psnr_comparison.png
â”‚   â”œâ”€â”€ ssim_comparison.png
â”‚   â”œâ”€â”€ depth_error_maps/
â”‚   â””â”€â”€ statistical_tests.csv
â”‚
â”œâ”€â”€ training_logs/            â† TensorBoard logs (50MB)
â”‚   â””â”€â”€ [.tfevents files]
â”‚
â””â”€â”€ raw_data/                 â† NeRF Synthetic dataset (2.5GB)
    â””â”€â”€ nerf_synthetic/

## ğŸ“‚ **Repository Structure**

```
depth-supervised-nerf-notebooks/
â”œâ”€â”€ README.md                               â† This file
â”œâ”€â”€ THEORY.md                               â† Detailed math/theory
â”œâ”€â”€ LICENSE                                 â† MIT License
â”œâ”€â”€ requirements.txt                        â† Python dependencies
â”œâ”€â”€ .gitignore                              â† Git exclusions
â”œâ”€â”€ Dockerfile                              â† Docker image
â”œâ”€â”€ download_example_data.sh                â† Data script
â”‚
â”œâ”€â”€ 00_Setup_and_Dependencies.ipynb         â† Environment setup
â”œâ”€â”€ 01_Data_Loading.ipynb                   â† Dataset loading
â”œâ”€â”€ 02_Core_Components.ipynb                â† NeRF architecture
â”œâ”€â”€ 03_Baseline_NeRF.ipynb                  â† Experiment 1
â”œâ”€â”€ 04_Soft_Depth_Supervision.ipynb         â† Experiment 2
â”œâ”€â”€ 05_Hard_Depth_Sampling.ipynb            â† Experiment 3
â”œâ”€â”€ 06_Hybrid_Strategy.ipynb                â† Experiment 4
â”œâ”€â”€ 07_Comprehensive_Evaluation.ipynb       â† Results analysis
â”œâ”€â”€ 08_Rendering_and_Visualization.ipynb    â† Video generation
â”‚
â”œâ”€â”€ Analysis/                               â† Extended analysis
â”‚   â”œâ”€â”€ ablation_studies.ipynb
â”‚   â””â”€â”€ hyperparameter_sweep.ipynb
â”‚
â”œâ”€â”€ Experiments/                            â† Custom experiments
â”‚   â”œâ”€â”€ custom_scenes.ipynb
â”‚   â””â”€â”€ real_world_data.ipynb
â”‚
â”œâ”€â”€ data/                                   â† Datasets (git-ignored)
â”‚   â””â”€â”€ nerf_synthetic/lego/
â”‚
â””â”€â”€ results/                                â† Outputs (git-ignored)
    â”œâ”€â”€ baseline/
    â”œâ”€â”€ soft/
    â”œâ”€â”€ hard/
    â””â”€â”€ hybrid/
```

---

## ğŸ› **Troubleshooting**

**Issue: CUDA Out of Memory**

```python
# Solution: Reduce batch size
batch_rays = 512  # instead of 1024

# Use half-resolution
load_synthetic_split("train", half_res=True)

# Reduce samples per ray
N_samples = 32  # instead of 64
```

**Issue: Training Too Slow**

```bash
# Verify GPU usage
nvidia-smi  # Should show 90-100% utilization

# Check CUDA availability
python -c "import torch; print(torch.cuda.is_available())"
```

**Issue: Poor Convergence**

- Increase iterations: `iters = 50000`
- Try hybrid strategy (most robust)
- Check depth ground truth quality

**Original NeRF:**

```bibtex
@inproceedings{mildenhall2020nerf,
  title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and others},
  booktitle={ECCV},
  year={2020}
}
```


## ğŸ™ **Acknowledgments**

- **ITMO University** - Robotics and AI Master's Program
- **Course:** Machine Learning in Robotics
- **Original NeRF** - Mildenhall et al., ECCV 2020
- **NeRF Synthetic Dataset** - Authors of NeRF paper

---

**Last Updated:** December 13, 2025  
**Author:** Ayan Ali ([@ayanali827](https://github.com/ayanali827)),
Umer Ahmed Baig Mughal [umerahmedbaig7](https://github.com/umerahmedbaig7), 
Urwa [@UrwaMughal7](https://github.com/UrwaMughal7)
**Course:** Machine Learning in Robotics, ITMO University  
**Research Focus:** Depth-supervised Neural Radiance Fields
